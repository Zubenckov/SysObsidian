0. [[#ЗАДАНИЕ]]
1. [[#Базовая сетевая настройка]]
2. [[#Клиенты и сервера, сетевая настройка]]
3. [[#hostname, на все устройства]]
4. [[#DNS, настройка на CR SRV]]
5. [[#Полноценная настройка NFTables на CR-RTR и BR-RTR]]
	5.1 [[#NFTables, __временный__ NAT для теста]]
6. [[#StrongSwan]]
7. [[#Настройка GRE туннеля на CR-RTR и BR-RTR]]
8. [[#FRR на CR-RTR и BR-RTR]]
9. [[#Wireguard CR-RTR -> OUT-CLI]]
10. [[#isc-dhcp-server на br-rtr]]
11. [[#Централизованный сбор логов на BR-SRV2]]
12. [[#Docker на BR-SRV1]]
    12.1 [[#Squid Proxy - BR-SRV1]]
	12.2 [[#Python Приложения]]
13. [[#EasyRsa на CR-SRV]]
14. [[#ИТОГОВЫЙ ВИД docker-compose.yml]]
15. [[#Ansible на BR-SRV1]]
## ЗАДАНИЕ
```r
ЗАДАНИЕ:
Прежде чем начать:
	Прочитайте задание полностью. Обратите внимание на то, что задание написано не в хронологичесĸом порядĸе, последовательность действий для выполнения задания остается на ваше усмотрение. Распределите время работы таĸим образом, чтобы заработать маĸсимальное ĸоличество баллов. Во время выполнения задания руĸоводствуйтесь следующими полезными советами:
> Если вам необходимо установить пароль, но в задании не уĸазано, ĸаĸой именно – установите P@ssw0rd. • Если вы по ĸаĸим-либо причинам хотите использовать свои собственные учетные данные/адреса/dns имена – не забудьте уĸазать их в форме, предоставленной организаторами по оĸончании соревновательных дней. 
> Внимательно проверяйте свою работу. Если эĸсперты не смогут подĸлючится ĸ чемулибо по ĸаĸой-либо причине (сетевая недоступность, не подходят учетные данные) – проверĸа этого устройства производится не будет. 
> Перед сдачей работы проверьте, что все устройства и сервисы переживают перезагрузĸу. 
> Любые дополнительные ĸомментарии и пожелания на проверĸу вы можете оставить для эĸспертов в предоставленной организаторами форме. Ваши ĸомментарии гарантированно будут прочитаны перед проверĸой или во время проверĸи. 
> Если вы столĸнулись с непониманием пунĸта задания – спросите. 
> Вы можете расходовать ваше время таĸ, ĸаĸ вам этого хочется. Помните, что время, затраченное на ваши нужды, не ĸомпенсируется, за исĸлючением обеда и случаев проблем со стороны площадĸи.
> Если не уĸазано иное, доступ ĸ любому URL ожидается по стандартным портам. 
> В ĸачестве внешнего DNS-сервера используйте 10.150.1.1, обслуживающего зону tech.skills и запросы на другие DNS-сервера 
> Необходимые для выполнения работы материалы размещены на files.atomarena.ru
Каĸ оценивается работа:
	Каждый субĸритерий имеет приблизительно одинаĸовый вес. Пунĸты внутри ĸаждого ĸритерия имеют разный вес, в зависимости от сложности пунĸта и ĸоличества пунĸтов в субĸритерии. 
	Схема оценĸа построена таĸим образом, чтобы ĸаждый пунĸт оценивался тольĸо один раз. Например, в сеĸции «Базовая ĸонфигурация» предписывается настроить имена для всех устройств, однаĸо этот пунĸт будет проверен на трех случайных устройствах, одинаĸовых для ĸаждого участниĸа и определяемых группой проверĸи и оценен тольĸо 1 раз. Одинаĸовые пунĸты могут быть проверены и оценены больше, чем 1 раз, если для их выполнения применяются разные настройĸи или они выполняются на разных ĸлассах устройств.
> В L3 топологии есть ошибĸа: Сервер BR-SRV1 Должен иметь адрес 192.168.2.100, BR-SRV2 -- 192.168.2.200. Внутренний адрес BR-RTR -- 192.168.2.1
На выполнение работы даётся 15 часов. Желаем успехов!

ПОДГОТОВКА ИНФРАСТРУКТУРЫ
	Сĸонфигурируйте базовые параметры виртуальных машин для обеспечения идентифиĸации и сетевой связанности. Обязательно должно быть настроено полное доменное имя и IP адрес согласно топологии. Оĸружение виртуальных машин, таĸое ĸаĸ оболочĸи, теĸстовые редаĸторы, графичесĸий интерфейс и прочее не является обязательным и остается на усмотрение участниĸа.
Виртуальные машины должны иметь возможность связываться друг с другом по доменным именам. Перед сдачей работы на проверĸу убедитесь, что с ĸаждой виртуальной машины возможен доступ по доменному имени ĸ любому сервису предприятия или ĸ любой другой виртуальной машине.
	На ĸаждую виртуальную машину обеспечьте доступ по протоĸолу SSH с использованием парольной аутентифиĸации для обычного пользователя, созданного по умолчанию. Доступ до виртуальных машин с использованием пользователя root должен быть запрещен. Если вам необходимо разрешить доступ под пользователем root – запретите парольный доступ и используйте сгенерированную вами пару ĸлючей. Ключи следует расположить в стандартной диреĸтории.
Обеспечьте наличие на ĸаждой ĸлиентсĸой машине утилит curl , wget и host.

НАСТРОЙКА СЕТЕВОЙ СВЯЗАННОСТИ:
1. Сĸонфигурируйте DHCP сервер на пограничном маршрутизаторе офиса BR в соответствии с требованиями:
   i. Диапазон выдаваемых адресов: 192.168.2.50-192.168.2.99.
   ii. Шлюз по умолчанию: адрес пограничного маршрутизатора.
   iii. DNS сервер: адрес BR-SRV1.
   iv. Клиентсĸий ĸомпьютер BR-CLI должен получать адрес по DHCP.
   v. В ĸачестве паĸета используйте isc-dhcp-server.
2. Выполните настройĸу DNS сервера на CR-SRV
   i. Сĸонфигурируйте зону stud.skills, записи добавьте согласно приложению 1.
   ii. Все запросы ĸ неизвестным зонам должны быть перенаправлены на сервер 10.150.1.1.
   iii. Сĸонфигурируйте подчиненный DNS сервер на BR-SRV1.
   iv. Обеспечьте автоматичесĸое обновление записи BR-CLI при смене IP-адреса.
   ▪ Обновлятся должна ĸаĸ прямая, таĸ и обратная зона.
3. На пограничных маршрутизаторах выполните настройĸу межсетевого эĸрана, с использованием паĸета nftables.
   i. Все устройства обоих офисов должны иметь доступ в Интернет.
   ii. Запретите трафиĸ для всех портов и протоĸолов, ĸроме необходимых для ĸорреĸтной работы инфраструĸтуры.
   iii. Запретите icmp трафиĸ на пограничные маршрутизаторы из вешних сетей. Трафиĸ из внутренних сетей обоих офисов должен быть разрешен.
   iv. Обеспечьте доступ по SSH ĸ серверу CR-SRV из внешних сетей. В ĸачестве внешнего порта используйте 2222 на маршрутизаторе CR-RTR.
4. Выполните передачу ssh ĸлючей с OUT-CLI на CR-SRV, для обеспечения беспарольной аутентифиĸации при подĸлючении на порт 2222 маршрутизатора CRRTR:
   i. Пользователь root на OUT-CLI должен иметь возможность беспарольного входа на CR-SRV под учетной записью пользователя administrator.
5. Для передачи данных между офисами организовано защищенное VPN соединение с использованием GRE и Strongswan.
   i. Необходимо восстановить работоспособность соединения.
   ii. Соединение должно быть защищено.
   iii. Для организации маршрутизации используйте протоĸол OSPF, реализованный паĸетом FRR
6. На маршрутизаторе CR-RTR сĸонфигурируйте сервер удаленного доступа Wireguard.
   i. В ĸачестве ĸлиента используйте OUT-CLI.
   ii. Клиент должен устанавливать соединение при загрузĸе ОС и иметь доступ ĸо всем ресурсам обоих офисов с использованием DNS имен.
7. Для подĸлючения OUT-CLI ĸ интернету настройте PPPoE соединение с провайдером. Используйте логин pppoeuser и пароль pppoepass для аутентифиĸации.
   
НАСТРОЙКА СЕРВИСОВ ЦЕНТРАЛИЗОВАННОГО УПРАВЛЕНИЯ
1. На CR-SRV разверните службу ĸаталога на базе ALD Pro для домена stud.skills.
   i. Пользователей необходимо импортировать из файла users.csv.
    a. В ĸачестве пароля всем пользователям установите P@ssw0rd
    b. При необходимости смены пароля при входе используйте P@ssw0rd1
   ii. Все машины с префиĸсом -CLI в названии должны иметь возможность аутентифиĸации через ALD Pro.
   iii. Обеспечьте доступ ĸ WEB интерфейсу ALD Pro по имени ald.stud.skills. В ĸачестве административной учетной записи используйте admin с паролем P@ssw0rd.
   iv. Сĸонфигурируйте права пользователей следующим образом:
	   a. На BR-CLI могут аутентифицироваться тольĸо пользователи группы branch и лоĸальные пользователи.
	   b. На OUT-CLI имеют право аутентифицироваться тольĸо пользователи группы remotes и лоĸальные пользователи
	   c. Пользователи группы main должны иметь возможность повышать привилегии с использованием sudo. У других доменных пользователей таĸой возможности быть не должно.
	   d. Пользователи группы branch должны иметь возможность повышать привилегии для выполнения ограниченного набора ĸоманд: cat, grep, head, tail, id.
	   e. Все пользователи, ĸоторым разрешен доступ ĸ sudo должны иметь возможность выполнения ĸоманд с повышенными привилегиями без ввода пароля.
   v. Для всех пользователей домена должны быть реализованы общие ĸаталоги.
       a. Каталог Share: все пользователи имеют права для чтения и записи в данный ĸаталог. При этом, удалять файлы из этого ĸаталога могут тольĸо владельцы файлов. Каталог должен быть смонтирован на рабочий стол всех доменных пользователей.
       b. Каталог Docs: доступ для чтения и записи имеет тольĸо группа branch. Остальные пользователи имеют доступ в формате read only. Каталог должен быть смонтирован на рабочий стол всех доменных пользователей.
       c. Для размещения ĸаталогов используйте сервер CR-SRV 
	    ▪ Каталог share должен быть доступен по пути /opt/share/ 
		▪ Каталог docs должен быть доступен по пути /opt/docs/ 
		▪ Убедитесь, что ĸаталоги на момент проверĸи не пусты
   vi. Реализуйте механизм инвентаризации машин с префиĸсом CLI в названии через Ansible на BR-SRV1:
	   a. Плейбуĸ должен собирать информацию о рабочих местах: 
		   ▪ Имя ĸомпьютера 
		   ▪ IP-адрес ĸомпьютера
	   b. Рабочий ĸаталог ansible должен располагаться в /etc/ansible
	   c. Отчеты, собранные с машин, разместите в папĸе reports рабочего ĸаталога Ansible.
		   a. Используйте формат .yml.
		   b. Имя файла должно совпадать с именем хоста, с ĸоторого собрана информация.
		   c. Убедитесь, что на момент проверĸи присутствует хотя-бы один файл журнала.
	   d. Файл с плейбуĸом должен быть доступен по пути /etc/ansible/ get_info.yml
	   e. В ĸачестве inventory файла используйте /etc/ansible/inventory.yml
		   ▪ Вся необходимая для подĸлючения информация должна содержаться в этом файле

НАСТРОЙКА СЕРВИСОВ
1. Реализуйте центр сертифиĸации на сервере CR-SRV.
   i. В ĸачестве CN уĸажите STUD-CA.
   ii. Все хосты должны доверять данному центру сертифиĸации на уровне системы, для работы утилит curl и wget. На ĸлиентсĸих ПК дополнительно необходимо обеспечить доверие ĸ сертифиĸату браузера firefox для всех пользователей.
   iii. Все сервисы, предусматривающие доступ через web-браузер, должны быть защищены с использованием сертифиĸатов от данного центра сертифиĸации (исĸлючением является ALD Pro, веб интерфейс ALD Pro защищать сертифиĸатом необязательно).
   iv. Все данные центра сертифиĸации должны быть расположены в диреĸтории / etc/ca
   v. Корневой сертифиĸат должен быть доступен по пути /etc/ca/cacert.pem
2. На сервере BR-SRV2 реализуйте централизованный сбор журналов.
   i. Журналы необходимо собирать с устройств CR-RTR и BR-RTR, в диреĸторию / opt/logs/.log.
   ii. Настройте ротацию логов: при достижении объёма в 10МБ журнал следует сжимать файл журнала. Выполнять ротацию следует раз в минуту.
   iii. Сĸонфигурируйте web сервер для доступа ĸ файлам журналов.
	   a. По ссылĸе https://logs.stud.skills пользователь должен видеть все содержимое папĸи /opt/logs и иметь возможность сĸачать любой из журналов.
	   b. При попытĸе подĸлючения по IP адресу пользователь должен получать ошибĸу 404.
	   c. При попытĸе доступа по http должно происходить перенаправление на https.
	   d. Защитите доступ ĸ сайту с использованием базовой аутентифиĸации, в ĸачестве логина используйте logadmin, в ĸачестве пароля – logpass.
3. Разверните орĸестратор Docker Compose на хосте BR-SRV1 для управления следующими сервисами:
   i. СУБД PostgresSQL
	   a. Создайте базу данных university.
	   b. Создайте струĸтуру БД:
		  a. Таблица marks - входит в группу таблиц learning.
		  b. Таблица students - входит в группу таблиц public
		  c. Таблица attendance - входит в группу таблиц accounting
		  d. Таблица subjects - входит в группу таблиц learning
		  e. Таблица reasons - входит в группу таблиц accounting
	   c. Создайте роли и пользователей, назначьте права на таблицы и группы:
		  a. Роль student - доступ тольĸо на чтение записей группы таблиц learning. Пользователь - sidorov
		  b. Роль teacher - доступ на создание и обновление записей таблиц marks, reasons и attendance. Пользователь - petrov
		  c. Роль administration - полный доступ ĸо всем таблицам БД. Пользователь - smirnov
	   d. Для создания таблиц используйте дамп database.sql
	   e. В ĸачестве паролей пользователей используйте P@ssw0rd
	   f. Обеспечьте на BR-SRV1 наличие ĸлиента для подĸлючения ĸ postgresql
   ii. Прямой проĸси-сервер SQUID
     a. Проĸси-сервер должен быть доступен по URL http://proxy.stud.skills:8081
     b. Сĸонфигурируйте хост BR-CLI для использования данного проĸси-сервера при доступе ĸ ресурсам для всех пользователей домена.
     c. Необходимо обеспечить использование проĸси тольĸо браузером Firefox, причём настройĸа должна применяться для всех пользователей, в том числе и доменных.
     d. Настройте ограничения доступа ĸ проĸси-серверу:
        a. Доступ ĸ сайту site2.tech.skills должен быть ограничен для всех ĸлиентсĸих хостов филиала.
        b. Доступ ĸ сайту site1.tech.skills должен предоставляться тольĸо после авторизации по логину и паролю. Используйте лоĸальную базу данных пользователей.
        c. Создайте пользователя mikhail с паролем proxypass . Данный пользователь должен получать доступ ĸ сайту site1.tech.skills.
   iii. Приложение на Python
     a. Выполните подготовĸу оĸружения с зависимостями. Используйте Python 3.12.
     b. Приложение должно быть доступно по адресу https://api.stud.skills:8443.
     c. Разверните два эĸземпляра приложения с реализацией балансировĸи при помощи HAProxy.
     d. Наименование эĸземпляра уĸажите в переменной оĸружения INSTANCE_ID.
     e. Исходный ĸод приложения изменять нельзя.
   iv. Сервисы должны быть доступны даже после перезагрузĸи хоста.
   v. В ĸачестве рабочей диреĸтории используйте /opt/compose/
По оĸончании выполнения задания:
Все виртуальные машины будут перезагружены путем симуляции сбоя по питанию. Сначала будут перезагружены сервера, затем ĸлиенты. Позабодьтесь о том, чтобы после перезагрузĸи инфраструĸтура была живой.      
```
![[L1.png]]
![[L3.png]]

---
> [!INFO ] Список имён DNS
> | Имя                    | Тип    | Адрес                                    |
> | ---------------------- | ------ | ---------------------------------------- |
> | 'hostname'.stud.skills | A, PTR | Адрес машины (для всех ВМ, кроме WIN-AD) |
> | api.stud.skills        | CNAME  | BR-SRV2                                  |
> | logs.stud.skills       | CNAME  | BR-SRV2                                  |
> | proxy.stud.skills      | CNAME  | BR-SRV2                                  |
> | ald.stud.skills        | CNAME  | CR-SRV                                   |

---
> [!BUG] Используемые версии ОС
| Имя хоста               | Операционная система  |
| ----------------------- | --------------------- |
| *-CLI                   | Astra Linux 1.8.1 GUI |
| CR-DB, BR-SRV1, BR-SRV2 | Astra Linux 1.8.1 CLI |
| CR-RTR, BR-RTR, ISP     | Debian 12.5           |
| CR-SRV                  | Astra Linux 1.7.5 CLI |
| WIN-AD                  | Windows Server 2022   |

---
> [!WARNING] Таблица адресации
| Имя устройства | IP-Адрес      | Маска |
| :------------: | ------------- | ----- |
|      ISP       | 1.2.1.1       | 29    |
|                | 2.2.1.1       | 29    |
|                | 3.10.10.1     | 24    |
|     CR-RTR     | 2.2.1.2       | 29    |
|                | 192.168.3.1   | 24    |
|     BR-RTR     | 1.2.1.2       | 29    |
|                | 192.168.2.1   | 24    |
|     BR-CLI     | DHCP          | 24    |
|    BR-SRV1     | 192.168.2.100 | 24    |
|    BR-SRV2     | 192.168.2.200 | 24    |
|     CR-SRV     | 192.168.3.10  | 24    |
|     CR-DB      | 192.168.3.200 | 24    |
|     CR-CLI     | 192.168.3.100 | 24    |

---
#НачалоНастройки
## Базовая сетевая настройка:
CR-RTR:
```bash
auto enp1s0
iface enp1s0 inet static
    address 1.2.1.2
    netmask 255.255.255.248
    gateway 1.2.1.1
    
auto enp6s0
iface enp6s0 inet static
    address 192.168.3.1
	netmask 255.255.255.0
```
BR-RTR:
```bash
auto enp1s0
iface enp1s0 inet static
    address 2.2.1.2
    netmask 255.255.255.248
    gateway 2.2.1.1
    
auto enp6s0
iface enp6s0 inet static
    address 192.168.2.1
	netmask 255.255.255.0
```
На обоих роутерах:
```bash
echo 'net.ipv4.ip_forward=1' | sudo tee -a /etc/sysctl.conf
```
---
## Клиенты и сервера, сетевая настройка
CR-SRV
```bash
auto eth0
iface eth0 inet static
    address 192.168.3.10
    netmask 255.255.255.0
    gateway 192.168.3.1
    dns-nameservers 192.168.3.10
```
BR-SRV1
```bash
auto enp1s0
iface enp1s0 inet static
    address 192.168.2.100
    netmask 255.255.255.0
    gateway 192.168.2.1
    dns-nameservers 192.168.3.10
```
BR-SRV2
```bash
auto enp1s0
iface enp1s0 inet static
    address 192.168.2.200
    netmask 255.255.255.0
    gateway 192.168.2.1
    dns-nameservers 192.168.3.10
```
OUT-CLI
```bash
auto eth0
iface eth0 inet static
    address 3.10.10.10
    netmask 255.255.255.0
    gateway 3.10.10.1
    dns-nameservers 192.168.3.10
```

На всех устройствах:
```bash
systemctl restart networking
```
---
## hostname, на все устройства:
```bash
# Офис HQ
hostnamectl set-hostname cr-rtr.stud.skills; exec bash
hostnamectl set-hostname cr-srv.stud.skills; exec bash
hostnamectl set-hostname cr-db.stud.skills; exec bash
# OUT
hostnamectl set-hostname out-cli.stud.skills; exec bash
# Офис BR
hostnamectl set-hostname br-rtr.stud.skills; exec bash
hostnamectl set-hostname br-srv1.stud.skills; exec bash
hostnamectl set-hostname br-srv2.stud.skills; exec bash
hostnamectl set-hostname br-cli.stud.skills; exec bash
```
---
### DNS, настройка на CR SRV:
```bash
apt update && apt install bind9 bind9utils bind9-doc -y
```
### Настройка named.conf.options и named.conf.local
```bash
vim /etc/bind/named.conf.options

# Содержимое:
options {
    directory "/var/cache/bind";
    
    forwarders {
        10.150.1.1;
    };
    
    dnssec-validation auto;
    
    listen-on { any; };
    listen-on-v6 { any; };
    
    allow-query { any; };
    
    recursion yes;
    
    allow-update { none; };
    
    version "not available";
};
---
vim /etc/bind/named.conf.local
# Содержимое
zone "stud.skills" {
    type master;
    file "/etc/bind/db.stud.skills";
    allow-transfer { 192.168.2.100; }; // BR-SRV1
    allow-update { 192.168.2.0/24; };
};

zone "3.168.192.in-addr.arpa" {
    type master;
    file "/etc/bind/db.192.168.3";
    allow-transfer { 192.168.2.100; };
    allow-update { 192.168.2.0/24; };
};

zone "2.168.192.in-addr.arpa" {
    type master;
    file "/etc/bind/db.192.168.2";
    allow-transfer { 192.168.2.100; };
};
---
systemctl restart bind9
systemctl enable bind9
```

Настройка зон:
### Зона stud.skills
```bash
vim /etc/bind/db.stud.skills
# Содержимое
$TTL    604800
@       IN      SOA     cr-srv.stud.skills. admin.stud.skills. (
                              2024010101    ; Serial
                          604800     ; Refresh
                           86400     ; Retry
                        2419200     ; Expire
                          604800 )   ; Negative Cache TTL

; NS records
@       IN      NS      cr-srv.stud.skills.
@       IN      NS      br-srv1.stud.skills.

; A records
cr-srv      IN      A       192.168.3.10
cr-db       IN      A       192.168.3.200
cr-cli      IN      A       192.168.3.100
cr-rtr      IN      A       192.168.3.1

br-srv1     IN      A       192.168.2.100
br-srv2     IN      A       192.168.2.200
br-rtr      IN      A       192.168.2.1
br-cli      IN      A       192.168.2.50
; br-cli будет динамическим

out-cli     IN      A       3.10.10.10
isp         IN      A       2.2.1.1

; CNAME records
api         IN      CNAME   br-srv2
logs        IN      CNAME   br-srv2
proxy       IN      CNAME   br-srv2
ald         IN      CNAME   cr-srv
```
### Зона db.192.168.3:
```bash
vim /etc/bind/db.192.168.3
# Содержимое
$TTL    604800
@       IN      SOA     cr-srv.stud.skills. admin.stud.skills. (
                              2024010101    ; Serial
                          604800     ; Refresh
                           86400     ; Retry
                        2419200     ; Expire
                          604800 )   ; Negative Cache TTL

@       IN      NS      cr-srv.stud.skills.
@       IN      NS      br-srv1.stud.skills.

10      IN      PTR     cr-srv.stud.skills.
200     IN      PTR     cr-db.stud.skills.
100     IN      PTR     cr-cli.stud.skills.
1       IN      PTR     cr-rtr.stud.skills.
```
### Зона db.192.168.2:
```bash
vim /etc/bind/db.192.168.2
$TTL    604800
@       IN      SOA     cr-srv.stud.skills. admin.stud.skills. (
                              2024010101    ; Serial
                          604800     ; Refresh
                           86400     ; Retry
                        2419200     ; Expire
                          604800 )   ; Negative Cache TTL

@       IN      NS      cr-srv.stud.skills.
@       IN      NS      br-srv1.stud.skills.

100     IN      PTR     br-srv1.stud.skills.
200     IN      PTR     br-srv2.stud.skills.
1       IN      PTR     br-rtr.stud.skills.
50      IN      PTR     br-cli.stud.skills.
```
---
## Настройка SSH на всех устройствах
```bash
vim /etc/ssh/sshd_config
# 1 CR-RTR, BR-RTR
# Содержимое
Port 22
PermitRootLogin no
PasswordAuthentication yes
PubkeyAuthentication yes
X11Forwarding no
AllowUsers user administrator

# CR-SRV, BR-SRV1, BR-SRV2, CR-DB, CLI машины
# Содержимое
Port 22
PermitRootLogin no
PasswordAuthentication yes
PubkeyAuthentication yes
X11Forwarding no
PrintMotd yes

systemctl restart ssh
systemctl enable ssh
```
---
## NFTables, __временный__ NAT для теста:
```bash
nft add table inet nat
nft add chain inet nat postrouting { type nat hook postrouting priority 100\; }
nft add rule inet nat postrouting oifname "enp1s0" masquerade
# Сохранение правил
nft list ruleset | sudo tee /etc/nftables.conf
systemctl enable nftables
systemctl restart nftables
# Проверка
nft list ruleset
```
---
## Полноценная настройка NFTables на CR-RTR и BR-RTR:
CR-RTR:
```bash
vim /etc/nftables.conf

# Содержимое
#!/usr/sbin/nft -f

flush ruleset

table inet filter {
    chain input {
        type filter hook input priority 0; policy drop;
        
        # Разрешаем loopback
        iifname "lo" accept
        ct state {established, related} accept
        iifname "tunnel0" accept
        # Разрешаем SSH на маршрутизатор
        tcp dport 22 accept
        ip saddr { 2.2.1.0/29 1.2.1.0/29 } icmp type {  echo-request, echo-reply } accept
        # Разрешаем ICMP из внутренних сетей
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } icmp type { echo-request, echo-reply } accept
        
        # Запрещаем ICMP из внешних сетей
        icmp type { echo-request, echo-reply } drop

        ip protocol ospf accept
    }
    
    chain forward {
        type filter hook forward priority 0; policy drop;
        ct state {established, related} accept
        ip saddr 10.255.255.0/30 accept
        ip daddr 10.255.255.0/30 accept
        # Разрешаем трафик между сетями
        ip saddr 192.168.3.0/24 ip daddr 192.168.2.0/24 accept
        ip saddr 192.168.2.0/24 ip daddr 192.168.3.0/24 accept
        
        # Разрешаем интернет для внутренних сетей
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } oifname "enp1s0" accept
    }
    
    chain output {
        type filter hook output priority 0; policy accept;
    }
}

table ip nat {
    chain postrouting {
        type nat hook postrouting priority 100; policy accept;
        oifname "enp1s0" masquerade
    }
    
    chain prerouting {
        type nat hook prerouting priority -100; policy accept;
        iifname "enp1s0" tcp dport 2222 dnat to 192.168.3.10:22
    }
}
```

BR-RTR
```bash
#!/usr/sbin/nft -f

flush ruleset

table inet filter {
    chain input {
        type filter hook input priority 0; policy drop;
        
        iifname "lo" accept
        ct state {established, related} accept
        iifname "tunnel0" accept
        # SSH на маршрутизатор
        tcp dport 22 accept
        ip saddr { 2.2.1.0/29 1.2.1.0/29 } icmp type {  echo-request, echo-reply } accept
        # ICMP только из внутренних сетей
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } icmp type { echo-request, echo-reply } accept
        icmp type { echo-request, echo-reply } drop

        ip protocol ospf accept
    }
    
    chain forward {
        type filter hook forward priority 0; policy drop;
        ct state {established, related} accept
        ip saddr { 2.2.1.2 1.2.1.2 } ip daddr { 2.2.1.2 1.2.1.2 } ip protocol 47 accept
        ip saddr 10.255.255.0/30 accept
        ip daddr 10.255.255.0/30 accept
        # Трафик между сетями
        ip saddr 192.168.3.0/24 ip daddr 192.168.2.0/24 accept
        ip saddr 192.168.2.0/24 ip daddr 192.168.3.0/24 accept
        
        # Интернет для внутренних сетей
        ip saddr { 192.168.3.0/24, 192.168.2.0/24 } oifname "enp1s0" accept
    }
    
    chain output {
        type filter hook output priority 0; policy accept;
    }
}

table ip nat {
    chain postrouting {
        type nat hook postrouting priority 100; policy accept;
        oifname "enp1s0" masquerade
    }
}
```
Применяем правила:
```bash
nft -c -f /etc/nftables.conf # проверка синтаксиса
nft -f /etc/nftables.conf # применяем, если все в норме
systemctl enable nftables
systemctl restart nftables
# Проверяем
nft list ruleset
```
---
## StrongSwan
```bash
apt install strongswan strongswan-pki -y
```
Переходим в конфиг (CR-RTR):
```bash
vim /etc/ipsec.conf
# Содержимое
config setup
	charondebug="ike 1, knl 1, cfg 0"

conn %default
	ikelifetime=60m
	keylife=20m
	rekeymargin=3m
	keyingtries=1
	keyexchange=ikev2
	authby=secret

conn gre-protect
	left=10.255.255.2
	right=10.255.255.1
	leftprotoport=gre
	rightprotoport=gre
	type=tunnel
	auto=start
	authby=secret
	keyexchange=ikev2
```
Переходим в конфиг (BR-RTR):
```bash
vim /etc/ipsec.conf
# Содержимое
config setup
	charondebug="ike 1, knl 1, cfg 0"

conn %default
	ikelifetime=60m
	keylife=20m
	rekeymargin=3m
	keyingtries=1
	keyexchange=ikev2
	authby=secret

conn gre-protect
	left=10.255.255.1
	right=10.255.255.2
	leftprotoport=gre
	rightprotoport=gre
	type=tunnel
	auto=start
	authby=secret
	keyexchange=ikev2
```
На обоих устройствах, меняем адреса на туннельные и ставим свой пароль:
```bash
vim /etc/ipsec.secrets
# Содержимое
10.255.255.1 10.255.255.2 : PSK "P@ssw0rd"
```
Перезагружаем службу:
```bash
ipsec reload
ipsec restart
systemctl restart strongswan-starter.service
```

---
## Настройка GRE туннеля на CR-RTR и BR-RTR:
```bash
CR-RTR
vim /etc/network/interfaces
# Содержимое
auto tunnel0
iface tunnel0 inet tunnel
    address 10.255.255.1
    netmask 255.255.255.252
    mode gre
    local 1.2.1.2
    endpoint 2.2.1.2
    ttl 255

BR-RTR
# Содержимое
auto tunnel0
iface tunnel0 inet tunnel
    address 10.255.255.2
    netmask 255.255.255.252
    mode gre
    local 2.2.1.2
    endpoint 1.2.1.2
    ttl 255
    
/etc/modules
> ip_gre
```
---
## FRR на CR-RTR и BR-RTR
```bash
apt install frr -y
vim /etc/frr/daemons
> ospfd = yes
# CR-RTR
vtysh
> conf t
> router ospf
> network 192.168.3.0/24 area 0
> network 10.255.255.0/30 area 0
> passive interface default
interface tunnel 0
> no ip ospf passive
# BR-RTR
vtysh
> conf t
> router ospf
> network 192.168.2.0/24 area 0
> network 10.255.255.0/30 area 0
> passive interface default
interface tunnel 0
> no ip ospf passive
# Общие команды проверки
> show ip ospf neighbor
> show ip ospf route
> show ip route ospf
```
---
## Wireguard CR-RTR -> OUT-CLI

CR-RTR и OUT-CLI:
```bash
mkdir -p /etc/wireguard
cd /etc/wireguard
wg genkey | sudo tee private.key | sudo wg pubkey | sudo tee public.key
chmod 600 private.key
# Смотрим публичный ключ (запомним для OUT-CLI)
cat public.key
```

Конфиг CR-RTR:
```bash
vim /etc/wireguard/wg0.conf
# Содержимое
[Interface]
PrivateKey = <содержимое_private.key_CR-RTR>
Address = 10.10.10.1/24
ListenPort = 51820
[Peer]
PublicKey = <содержимое_public.key_OUT-CLI>
AllowedIPs = 10.10.10.2/32
```
Конфиг OUT-CLI:
```bash
vim /etc/wireguard/wg0.conf
# Содержимое
[Interface]
PrivateKey = <содержимое_private.key_CR-RTR>
Address = 10.10.10.2/30

[Peer]
PublicKey = <содержимое_public.key_OUT-CLI>
Endpint = 2.2.1.2:51820
AllowedIPs = 0.0.0.0/0
PersistentKeepalive = 25
```
Для обоих устройств:
```bash
systemctl enable wg-quick@wg0
systemctl start wg-quick@wg0
systemctl restart wg-quick@wg0
```
---
## isc-dhcp-server на br-rtr
В самом начале устанавливаем пакет:
```bash
apt update && apt install isc-dhcp-service
```
Переходим в конфигурационный файл:
```bash
/etc/dhcp/dhcpd.conf
# Содержимое:
option domain-name "stud.skills";
option domain-name-servers 192.168.2.100;

default-lease-time 600;
max-lease-time 7200;
authoritative;

subnet 192.168.2.0 netmask 255.255.255.0 {
	range 192.168.2.50 192.168.2.99;
	option routers 192.168.2.1;
	option subnet-mask 255.255.255.0;
	option domain-name-servers 192.168.2.100;
	ddns-domainname "stud.skills.";
	ddns-rev-domainname "in-addr.arpa.";
	update-static-leases on;
	update-conflict-detection off;
}

# Включаем и перезагружаем службу:
systemctl enable isc-dhcp-service
systemctl restart isc-dhcp-service
# После чего, проверяем статус:
systemctl status isc-dhcp-service
```
## Централизованный сбор логов на BR-SRV2
```bash
apt update && apt install rsyslog -y
```
Переходим к ротации:
```bash
vim /etc/logrotate.d/remote-logs
# Содержимое
/opt/logs/*.log {
	missingok
	rotate 5
	compress
	delaycompress
	size 10M
	copytruncate
	create 644 syslog syslog
	postrotate
		/usr/lib/rsyslog/rsyslog-rotate
	endscript
}
```
Идем в конфиг файл:
```bash
# Раскоментируем или добавим:
module(load="imudp")
input(type="imudp" port="514")

module(load="imtcp")
input(type="imtcp" port="514")
```
Создаем файлик:
```bash
vim /etc/rsyslog.d/50-default.conf
# Содержимое
$template RemoteLogs,"/opt/logs/%HOSTNAME%.log"
*.* ?RemoteLogs

:fronthost-ip, isequal, "192.168.3.1" -
:fronthost-ip, isequal, "192.168.2.1" -
```
Создание пользователя, и директории для логов:
```bash
# Создаем группу и пользователя
groupadd -r syslog
useradd -r -s /bin/false -g syslog -c "System Logging User" syslog
# Создаем директорию и присваиваем ей права
mkdir -p /opt/logs
chown syslog:syslog /opt/logs
```
На CR-RTR и BR-RTR, устанавливаем rsyslog и создаем файл:
```bash
vim /etc/rsyslog.d/50-forward.conf
# Содержимое везде одинаковое
*.* @192.168.2.200:514
```
Запускаем службу (везде) и перезагружаем:
```bash
systemctl enable rsyslog
systemctl restart rsyslog
```
Проверяем:
```bash
# На BR-SRV2 смотрим файлы логов:
ls -la /opt/logs
# Должны появится br-rtr.log и cr-rtr.log

# Идем к примеру на CR-RTR и пишем следующее:
logger "Test log"

# Возвращаемся на BR-SRV2, проверяем:
tail -f /opt/logs/cr-rtr.log
> Вывод: <Пользователь>: Test log
```

Настройка Web-доступа к логам на BR-SRV2:
```bash
apt install nginx apache2-utils -y 
# После установки, сразу же создаем пользователя
htpasswd -bc /etc/nginx/.htpasswd logadmin logpass
```
Создаем приватный ключ для nginx:
```bash
# На BR-SRV2 создаем самоподписанный сертификат
sudo mkdir -p /etc/ssl/private
sudo mkdir -p /etc/ssl/certs
# Создаем приватный ключ
sudo openssl genrsa -out /etc/ssl/private/nginx-selfsigned.key 2048
# Создаем сертификат
sudo openssl req -new -x509 -key /etc/ssl/private/nginx-selfsigned.key \
    -out /etc/ssl/certs/nginx-selfsigned.crt \
    -days 365 -subj "/CN=logs.stud.skills"
```
Настраиваем права доступа:
```bash
chmod 600 /etc/ssl/private/nginx-selfsigned.key
chmod 644 /etc/ssl/certs/nginx-selfsigned.crt
```
Переходи в конфиг файл:
```bash
/etc/nginx/sites-available/logs
# Содержимое
server {
    listen 80;
    server_name _;
    return 404;
}
server {
	listen 80;
	server_name logs.stud.skills;
    return 301 https://$host$request_uri;
}
server {
    listen 443 ssl;
    server_name logs.stud.skills;
    ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt;
    ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key;
    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/.htpasswd;
    location / {
        root /opt/logs;
        autoindex on;
    }
}
```
Включаем сайт:
```bash
ln -s /etc/nginx/sites-available/logs /etc/nginx/sites-enabled/
nginx -t
systemctl restart nginx
```
Проверяем с любого ПК:
```bash
https://logs.stud.skills
# Логин: logadmin
# Пароль: logpass
```

## Docker на BR-SRV1
```bash
apt update && apt install docker.io docker-compose-v2 -y
# Добавляем пользователя в группу docker
usermod -aG docker $USER
newgrp docker
# Запускаем службу
sudo systemctl enable docker
sudo systemctl start docker
# Проверяем
docker --version
docker-compose --version
```
Создаем рабочую директорию:
```bash
mkdir -p /opt/compose
cd /opt/compose
vim docker-compose.yml
# Содержимое
version: '3.8'
services:
  postgres:
    image: postgres:13
    container_name: postgres-db
    environment:
      POSTGRES_DB: university
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: P@ssw0rd
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database.sql:/docker-entrypoint-initdb.d/database.sql
    restart: unless-stopped
    
volumes:
  postgres_data:
```
Создаем дамп базы данных
```sql
vim database.sql
# Содержимое
-- groups
CREATE SCHEMA IF NOT EXISTS learning;
CREATE SCHEMA IF NOT EXISTS accounting;
-- tables
CREATE TABLE public.students (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    group_name VARCHAR(50)
);
CREATE TABLE learning.subjects (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    credits INTEGER
);
CREATE TABLE learning.marks (
    id SERIAL PRIMARY KEY,
    student_id INTEGER REFERENCES public.students(id),
    subject_id INTEGER REFERENCES learning.subjects(id),
    mark INTEGER,
    date DATE
);
CREATE TABLE accounting.attendance (
    id SERIAL PRIMARY KEY,
    student_id INTEGER REFERENCES public.students(id),
    date DATE,
    present BOOLEAN
);
CREATE TABLE accounting.reasons (
    id SERIAL PRIMARY KEY,
    attendance_id INTEGER REFERENCES accounting.attendance(id),
    reason TEXT
);
-- roles
CREATE ROLE student;
CREATE ROLE teacher;  
CREATE ROLE administration;
-- user
CREATE USER sidorov WITH PASSWORD 'P@ssw0rd';
CREATE USER petrov WITH PASSWORD 'P@ssw0rd';
CREATE USER smirnov WITH PASSWORD 'P@ssw0rd';
--grants
GRANT student TO sidorov;
GRANT teacher TO petrov;
GRANT administration TO smirnov;
GRANT USAGE ON SCHEMA learning TO student;
GRANT SELECT ON ALL TABLES IN SCHEMA learning TO student;
GRANT USAGE ON SCHEMA learning, accounting TO teacher;
GRANT SELECT, INSERT, UPDATE ON learning.marks TO teacher;
GRANT SELECT, INSERT, UPDATE ON accounting.reasons TO teacher;
GRANT SELECT, INSERT, UPDATE ON accounting.attendance TO teacher;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public, learning, accounting TO administration;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public, learning, accounting TO administration;
-- test data
INSERT INTO public.students (name, group_name) VALUES 
('Алексей Иванов', 'Группа 101'),
('Егор Наградов', 'Группа 102');
INSERT INTO learning.subjects (name, credits) VALUES
('Математика', 5),
('Физика', 4);
```
Запуск docker compose:
```bash
cd /opt/compose
docker compose up -d
# Проверяем
docker ps
docker logs postgres-db
```
Проверяем базы данных:
```bash
# Установка клиента PostgreSQL
apt install postgresql-client -y

# Подключение к базе
psql -h localhost -U sidorov -d university -W
# Пароль: P@ssw0rd

# Проверяем доступ
\dt learning.*
\dt accounting.*
\dt public.*
```
Возможные исправления ошибок:
```bash
# Проверяем что контейнер запущен
docker ps
# Смотрим логи PostgreSQL
docker logs postgres-db
# Подключаемся правильно
psql -h localhost -U postgres -d university -W
# Пароль: P@ssw0rd

# Проверяем что пользователи создались
\du
```
Проверка Postgres:
```bash
# Подключаемся как postgres
psql -h localhost -U postgres -d university -W
# Пароль: P@ssw0rd

# Проверяем пользователей
\du

# Пробуем подключиться как smirnov
psql -h localhost -U smirnov -d university -W
# Пароль: P@ssw0rd

# Должны видеть все таблицы
\dt *.*
```
## Squid Proxy - BR-SRV1
Переходим в ранее созданную /opt/compose:
```bash
cd /opt/compose
vim docker-compose.yml
# Дополняем файл:
version: '3.8'

services:
  postgres:
    image: postgres:13
    container_name: postgres-db
    environment:
      POSTGRES_DB: university
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: P@ssw0rd
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database.sql:/docker-entrypoint-initdb.d/database.sql
    restart: unless-stopped
	networks:
	  - backend
# Добавлено следующее (описано ниже):
  squid:
    image: sameersbn/squid:3.5.27-2
    container_name: squid-proxy
    ports:
      - "8081:3128"
    environment:
      - SQUID_PROXY_PORT=3128
    volumes:
      - ./squid.conf:/etc/squid/squid.conf
      - squid_cache:/var/spool/squid
    restart: unless-stopped
	networks:
	  - backend
	    
volumes:
  postgres_data:
  # также добавлен volume:
  squid_cache:

networks:
	backend:
		driver: bridge
```
Создаем конфиг для squid proxy:
```bash
vim squid.conf
# Содержимое
http_port 3128
visible_hostname proxy.stud.skills
cache_dir ufs /var/spool/squid 100 16 256
maximum_object_size 256 MB
acl local_net src 192.168.2.0/24
acl local_net src 192.168.3.0/24
acl site1 dstdomain site1.tech.skills
acl site2 dstdomain site2.tech.skills
auth_param basic program /usr/lib/squid/basic_ncsa_auth /etc/squid/passwd
auth_param basic realm Squid proxy
acl authenticated proxy_auth REQUIRED
http_access allow local_net site2
http_access allow authenticated site1
http_access deny site2
http_access allow local_net
http_access deny all
```
Создаем файл с пользователями:
```bash
# Создаем пользователя mikhail
sh -c "echo 'mikhail:proxypass' > /opt/compose/squid.passwd"
# Хешируем пароль для Squid
apt update && apt install apache2-utils -y
htpasswd -b -c /opt/compose/squid.passwd mikhail proxypass
```
Включаем и проверяем:
```bash
# Включение
docker-compose up -d squid
# Проверка
docker ps
curl -I http://proxy.stud.skills:8081
```
Проблема с Squid:
```bash
# Проверяем контейнер Squid
docker ps | grep squid

# Смотрим логи Squid
docker logs squid-proxy

# Частая проблема - нет файла passwd
docker exec -it squid-proxy ls -la /etc/squid/
```
Проверка Squid:
```bash
# Тестируем proxy
curl -v --proxy http://localhost:8081 http://google.com

# Тестируем аутентификацию
curl -v --proxy http://mikhail:proxypass@localhost:8081 http://site1.tech.skills
```

## Python Приложения
```bash
# Добавляем сервисы в докер
python-app1:
	image: python:3.12-alpine
	container_name: python-app-1
	working-dir: /app
	volumes:
	  - "5001:5000"
	environment:
	  - INSTANCE_ID=app1
	command: sh -c "pip install -r requirments>txt && python app.py"
	restart: unless stopped
	networks:
	  - backend

python-app2:
	image: python:3.12-alpine
	container_name: python-app-2
	working-dir: /app
	volumes:
	  - "5002:5000"
	environment:
	  - INSTANCE_ID=app1
	command: sh -c "pip install -r requirments>txt && python app.py"
	restart: unless stopped
	networks:
	  - backend
	    
  haproxy-balancer:
    image: haproxy:2.8
    container_name: haproxy-balancer
    ports:
      - "8443:8443"
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
      - ./ssl/cert.pem:/usr/local/etc/haproxy/cert.pem
    depends_on:
      - python-app-1
      - python-app-2
    restart: unless-stopped
    networks:
      - backend
    
```
Создаем директорию:
```bash
mkdir app
vim app/app.py
# Содержимое:
from flask import Flask, jsonify
import os

app = Flask (__name__)

@app.route('/')
def hello():
	instance_id = os.getenv('INSTANCE_ID', 'unknown')
	return jsonify(
		'message': f'Hello from {instance_id}',
		'instance': instance_id

@app.route('/health')
def health():
	return jsonify({'status': 'healthy'})

if __name__ == '__main__';
	app.run(host='0.0.0.0', port=5000, debug=False)
```
Создаем файл requirments.txt:
```bash
app/requirments.txt
# Содерижое
Flask==3.0.0
```
Настраиваем HAProxy:
```bash
vim /opt/compose/haproxy.cfg
# Содержимое
global
    daemon
    maxconn 256
    log 127.0.0.1 local0

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    option forwardfor
    option httplog

frontend http_front
    bind *:8443 ssl crt /usr/local/etc/haproxy/cert.pem
    http-request set-header X-Forwarded-Proto https if { ssl_fc }
    default_backend http_back

backend http_back
    balance roundrobin
    server app1 python-app-1:5000 check
    server app2 python-app-2:5000 check

listen stats
    bind *:1936
    stats enable
    stats uri /
    stats hide-version
```
Создаем SSL сертификат для HAProxy:
```bash
mkdir -p ssl
cd ssl
sudo openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 \
    -keyout cert.key -out cert.crt \
    -subj "/CN=api.stud.skills"

# Объединяем в один файл для HAProxy
cat cert.crt cert.key > cert.pem
```
## EasyRsa на CR-SRV
```bash
apt install easy-rsa -y
# Создаем директорию
mkdir -p /etc/ca/easy-rsa
cd /etc/ca/easy-rsa
cp -r /usr/share/easy-rsa/* .
chmod 600 /etc/ssl/private/*.key
chown root:root /etc/ssl/private/*.key
```
Переходим к настройке VARS:
```bash
vim vars
# Содержимое
set_var EASYRSA_REQ_COUNTRY    "RU"
set_var EASYRSA_REQ_PROVINCE   "Moscow"
set_var EASYRSA_REQ_CITY       "Moscow"
set_var EASYRSA_REQ_ORG        "STUD-SKILLS"
set_var EASYRSA_REQ_EMAIL      "admin@stud.skills"
set_var EASYRSA_REQ_OU         "IT"
set_var EASYRSA_ALGO           "ec"
set_var EASYRSA_DIGEST         "sha512"
```

```bash
# Инициализируем PKI
./easyrsa init-pki

# Создаем корневой CA
./easyrsa build-ca nopass
# Common Name: STUD-CA

# Создаем сертификат для сервера
./easyrsa build-server-full br-srv2.stud.skills nopass
./easyrsa build-server-full api.stud.skills nopass
./easyrsa build-server-full logs.stud.skills nopass
./easyrsa build-server-full proxy.stud.skills nopass
./easyrsa build-server-full cr-srv.stud.skills nopass
./easyrsa build-client-full out-cli.stud.skills nopass
# Экспортируем сертификаты
cp pki/ca.crt /etc/ca/cacert.pem
cp pki/issued/br-srv2.stud.skills.crt /etc/ca/br-srv2.crt
cp pki/private/br-srv2.stud.skills.key /etc/ca/br-srv2.key
```
Для BR-SRV2 (логи):
```bash
mkdir -p /etc/ssl/private /etc/ssl/certs
cp pki/private/logs.stud.skills.key /etc/ssl/private/
cp pki/issued/logs.stud.skills.crt /etc/ssl/certs/
cp pki/ca.crt /etc/ssl/certs/stud-ca.crt
```
BR-SRV2, nginx:
```bash
vim /etc/nginx/sites-available/logs
# Обновляем секцию SSL
server {
    listen 443 ssl;
    server_name logs.stud.skills;
    
    ssl_certificate /etc/ssl/certs/logs.stud.skills.crt; # вносим изменения
    ssl_certificate_key /etc/ssl/private/logs.stud.skills.key; # вносим изменения
    ssl_trusted_certificate /etc/ssl/certs/stud-ca.crt; # добавляем
    
    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/.htpasswd;
    
    location / {
        root /opt/logs;
        autoindex on;
    }
}
```
Распространим корневой сертификат на все хосты (с CR-SRV):
```bash
scp /etc/ca/easy-rsa/pki/ca.crt user@br-srv1:/tmp/stud-ca.crt
scp /etc/ca/easy-rsa/pki/ca.crt user@br-srv2:/tmp/stud-ca.crt
scp /etc/ca/easy-rsa/pki/ca.crt user@br-cli:/tmp/stud-ca.crt
scp /etc/ca/easy-rsa/pki/ca.crt user@out-cli:/tmp/stud-ca.crt
# На каждой машине после копирования:
update-ca-certificates
# Проверяем работоспособность:
curl -v https://logs.stud.skills
```

### ИТОГОВЫЙ ВИД docker-compose.yml
```yml
version: '3.8'

services:
  postgres:
    image: postgres:13
    container_name: postgres-db
    environment:
      POSTGRES_DB: university
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: P@ssw0rd
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database.sql:/docker-entrypoint-initdb.d/database.sql
    restart: unless-stopped
    networks:
      - backend

  squid:
    image: sameersbn/squid:3.5.27-2
    container_name: squid-proxy
    ports:
      - "8081:3128"
    environment:
      - SQUID_PROXY_PORT=3128
    volumes:
      - ./squid.conf:/etc/squid/squid.conf
      - squid_cache:/var/spool/squid
    restart: unless-stopped
    networks:
      - backend

  python-app-1:
    image: python:3.12-alpine
    container_name: python-app-1
    working_dir: /app
    volumes:
      - ./app:/app
    ports:
      - "5001:5000"
    environment:
      - INSTANCE_ID=app1
    command: sh -c "pip install flask && python app.py"
    restart: unless-stopped
    networks:
      - backend

  python-app-2:
    image: python:3.12-alpine
    container_name: python-app-2
    working_dir: /app
    volumes:
      - ./app:/app
    ports:
      - "5002:5000"
    environment:
      - INSTANCE_ID=app2
    command: sh -c "pip install flask && python app.py"
    restart: unless-stopped
    networks:
      - backend

  haproxy-balancer:
    image: haproxy:2.8
    container_name: haproxy-balancer
    ports:
      - "8443:8443"
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
      - ./ssl/cert.pem:/usr/local/etc/haproxy/cert.pem
    depends_on:
      - python-app-1
      - python-app-2
    restart: unless-stopped
    networks:
      - backend

volumes:
  postgres_data:
  squid_cache:

networks:
  backend:
    driver: bridge
```
## Ansible на BR-SRV1
```bash
apt update && apt install ansible -y
mkdir -p /etc/ansible
cd /etc/ansible
```
Создаем inventory файл:
```bash
vim inventory.yml
# Содержимое:
all:
  hosts:
    br-cli.stud.skills:
      ansible_user: user
      ansible_ssh_pass: P@ssw0rd
      ansible_connection: ssh
    cr-cli.stud.skills:
      ansible_user: user  
      ansible_ssh_pass: P@ssw0rd
      ansible_connection: ssh
```
Создаем плейбук для сбора информации:
```bash
vim get_info.yml
# Содержимое:
---
- name: Collect client information
  hosts: all
  gather_facts: yes
  
  tasks:
    - name: Create reports directory
      file:
        path: /etc/ansible/reports
        state: directory
        mode: 0755
        
    - name: Collect system information
      setup:
        gather_subset:
          - network
          - hardware
          
    - name: Generate report
      copy:
        content: |
          hostname: {{ ansible_hostname }}
          ip_address: {{ ansible_default_ipv4.address }}
          mac_address: {{ ansible_default_ipv4.macaddress }}
          os: {{ ansible_distribution }} {{ ansible_distribution_version }}
        dest: "/etc/ansible/reports/{{ inventory_hostname }}.yml"
        mode: 0644
```
Запуск Ansible:
```bash
# Тестируем подключение
ansible -i inventory.yml all -m ping

# Запускаем плейбук
ansible-playbook -i inventory.yml get_info.yml

# Проверяем отчеты
ls -la /etc/ansible/reports/
cat /etc/ansible/reports/br-cli.stud.skills.yml
```
